{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486225e8-12a5-499f-a620-ea41ccef1815",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation and Chatbot Application\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augement the Large Models and enable us to perform tasks which meet our goals and enable our use-cases. At a high level Langchain has \n",
    "\n",
    "Data: Connect a language model to other sources of data\n",
    "Agent: Allow a language model to interact with its environment\n",
    "\n",
    "LangChain can be used in two major ways:\n",
    "\n",
    "<li>Indivisual Components: LangChain provides modular abstractions for the components neccessary to work with language models. LangChain also has collections of implementations for all these abstractions. The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.\n",
    "\n",
    "<li>Use-Case Specific Chains: Chains can be thought of as assembling these components in particular ways in order to best accomplish a particular use case. These are intended to be a higher level interface through which people can easily get started with a specific use case. These chains are also designed to be customizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789df3c-aff9-4957-a32d-80086b1f7ddb",
   "metadata": {},
   "source": [
    "## Topics covered:\n",
    "\n",
    "In this notebook we will be covering the below topics:\n",
    "\n",
    "- **LLM** Examine running an LLM in bare form to check for output\n",
    "- **Vector DB** Examine various vector databases like FAISS or CHROMA and leverage to produce better results using RAG\n",
    "- **Prompt template** Examine use of PROMPT Template\n",
    "- **Question Answering** Retrieval Augmented Generation (RAG)\n",
    "- **Chatbot** Build a Interactive Chatbot with Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1176e9-9a60-4713-b72f-9e54d2a259b8",
   "metadata": {},
   "source": [
    "## Key points for consideration\n",
    "\n",
    "1. Long Document that exceed the token limit? Ability to Chain , Mapo_reduce, Refine, Map-Rerank\n",
    "2. Cost of per token -- minimize the tokens and send in only relevant tokens to Model\n",
    "3. Which model to use --\n",
    "    - Cohere, AI21, Huggingface Hub, Manifest, Goose AI, Writer, Banana, Modal, StochasticAI, Cerebrium, Petals, Forefront AI, Anthropic, DeepInfra, and self-hosted Models.\n",
    "    - Example LLM cohere = Cohere(model='command-xlarge')\n",
    "    - Example LLM flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "4. Input Data Sources PDF, WebPages, CSV , S3, EFS\n",
    "5. Orchestration with External Tasks\n",
    "    - External Tasks - Agent SerpApi, SEARCH Engines\n",
    "    - Math Calculator\n",
    "6. Conversation Management and History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de785d0-3b27-4699-87be-a34484c429fa",
   "metadata": {},
   "source": [
    "### Key components of LangChain\n",
    "\n",
    "Let us examine the key components of Langchain. At the heart and the center is the Large Model.\n",
    "\n",
    "There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:\n",
    "\n",
    "**Models**: The various model types and model integrations LangChain supports.\n",
    "\n",
    "<img src='./images/models.png' width =\"500\"/>\n",
    "\n",
    "    \n",
    "**Prompts**: This includes prompt management, prompt optimization, and prompt serialization.\n",
    "    \n",
    "<img src=\"images/prompt.png\" width=\"500\"/>\n",
    "    \n",
    "**Memory**: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "    \n",
    "**Indexes**: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\n",
    "    \n",
    "<img src=\"images/vectorstore.png\" width=\"500\"/>\n",
    "\n",
    "**Chains**: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "<img src=\"images/chains.png\" width=\"500\"/>\n",
    "\n",
    "**Agents**: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
    "\n",
    "\n",
    "    \n",
    "**Callbacks**: It can be difficult to track all that occurs inside a chain or agent. Callbacks help add a level of observability and introspection.\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402714bf-14b6-4481-8e33-fc3d0b8a81f4",
   "metadata": {},
   "source": [
    "### Chat Bot key elements\n",
    "\n",
    "The first process in a chat bot is to generate embeddings. Typically you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using a GPT-J embeddings model for this\n",
    "\n",
    "<img src=\"images/Embeddings_lang.png\" width=\"600\"/>\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "<img src=\"images/Chatbot_lang.png\" width=\"600\"/>\n",
    "\n",
    "For processes which need deeper analysis, conversation history we will need to summarize every interaction to keep it succinct and for that we can follow this flow below which uses PineCone as an example\n",
    "\n",
    "For the various Tools which are available \n",
    "\n",
    "<img src=\"images/chatbot_internet.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e632c30-db64-4e60-9ce3-8fbcfb798e7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install langchain --quiet\n",
    "!pip install pypdf==3.8.1\n",
    "!pip install transformers==4.24.0\n",
    "!pip install sentence_transformers==2.2.2\n",
    "!pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841f947-6263-4735-8031-a69a7e4e4f4f",
   "metadata": {},
   "source": [
    "## Note\n",
    "You must Restart Kernel here for the installations to take effect. After restarting kernel, run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0182480-a2ad-4747-a2d6-c2d4f00f7f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"Region is {aws_region}, Role is {aws_role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bc009-dd0b-4839-95bd-5a9907a84887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To list all the available textgeneration models in JumpStart uncomment and run the code below\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "filter_value = \"task == llm\"\n",
    "\n",
    "print(\"===== Available Llama-2 Models =====\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "for model in text_generation_models:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18101583-5b29-4f1e-8d41-8777472f4d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'huggingface-llm-falcon-7b-instruct-bf16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f27e1-140e-4c80-b94f-6895cdc17f27",
   "metadata": {},
   "source": [
    "We will now deploy this model to a SageMaker endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264081d-7afc-47a4-bca6-9e9140bc7e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "try:\n",
    "    model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.2xlarge\")\n",
    "    predictor = model.deploy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb4b4e-02cf-40f0-b63e-19c53c52259b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "region = aws_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3808d-3f10-4b14-a31e-4eaea817d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"SageMaker Endpoint with Falcon-7b instruct deployed: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551a45e-0893-46e2-b278-fd3181112df7",
   "metadata": {},
   "source": [
    "## Simple Q&A with Falcon \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70d943-b373-4426-ae05-c0d866068a74",
   "metadata": {},
   "source": [
    "In order to use our model endpoint with LangChain we wrap up endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint` which is LangChain's built in support for SageMaker endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bb7d5-dd26-4a48-ad2c-4295587e418f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt,  \"parameters\": model_kwargs}) \n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41aee2-3b22-4ef8-8886-1686515effd8",
   "metadata": {},
   "source": [
    "Next, we will use LangChain PromptTemplate and LLMChain to create a prompt and invoke the model endpoint to get a response. We will use this method, or methods similar to this using LangChain throughout the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58438587-604f-4163-8f1f-dd977dc94524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(question=  \"What is the plot of 'The Expanse'?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ac293-d6a6-4e61-ad6f-18a10380d6bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contextual Q&A with Falcon-7b instruct\n",
    "---\n",
    "\n",
    "Given a context, ask Falcon-7b instruct to answer only from within that context. Let's create a prompt template for that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049f468-675b-4191-be30-4a592b14a609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant. Given a document, answer the 'Question'. Keep your answers strictly from within the document. \n",
    "If the answer to the question is not in the document, simplay say \"I do not know\", do not make up an answer.\n",
    "\n",
    "Document: {document}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e0998-deb9-4a2d-970c-bf26bf324cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document=\"\"\"The Expanse is a science fiction television series based on the novel series of the same name by James S. A. Corey (Daniel Abraham and Ty Franck). \\\n",
    "It was developed by Mark Fergus and Hawk Ostby, who served as executive producers alongside Naren Shankar, Andrew Kosove, Broderick Johnson, Laura Lancaster, \\\n",
    "Sean Daniel, Jason Brown, and Sharon Hall. The first season premiered on December 14, 2015, with the second season following on February 1, 2017. The third \\\n",
    "season premiered on April 11, 2018.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"When did the first season of 'The Expanse' premier?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb224a-3419-42dc-afd1-b3aa1bb26210",
   "metadata": {},
   "source": [
    "### Let's ask it something completely outside of the document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49386ce8-ea15-4804-ab33-6b876368d591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document=\"\"\"The Expanse is a science fiction television series based on the novel series of the same name by James S. A. Corey (Daniel Abraham and Ty Franck). \\\n",
    "It was developed by Mark Fergus and Hawk Ostby, who served as executive producers alongside Naren Shankar, Andrew Kosove, Broderick Johnson, Laura Lancaster, \\\n",
    "Sean Daniel, Jason Brown, and Sharon Hall. The first season premiered on December 14, 2015, with the second season following on February 1, 2017. The third \\\n",
    "season premiered on April 11, 2018.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"When was 'Breaking Bad' made?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1251cf-d0ed-4c15-933e-7f67abfbe6ff",
   "metadata": {},
   "source": [
    "We may see the model respond with an answer, which may be correct afterall, but the context doesn't include any details about the question asked. We can mitigate this with few shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de1934-6133-4d1d-beaf-0c142eb65a62",
   "metadata": {},
   "source": [
    "## Few shot Q&A\n",
    "---\n",
    "\n",
    "In this section we will perform \"few shot\" Q&A with the model. We will show it a few example and then ask it a question to be answered based on a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22dd602-e373-4e63-9eb5-2dc07db0e01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context: Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote \n",
    "what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, \n",
    "just characters with strong feelings, which I imagined made them deep.\n",
    "Question: What are the two things the author worked outside of school?\n",
    "Answer: Writing and programming\n",
    "===\n",
    "Context: The prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% .\n",
    "Anthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight .\n",
    "Question: What steps did the ministry of education take to address the issue?\n",
    "Answer: I do not know\n",
    "===\n",
    "Context: {document}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20d16b-6cc6-4ce2-bca2-16c3475e1a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document=\"\"\"In the quiet town of Willowbrook, elderly Ms. Agatha discovered a mysterious old key while tending to her garden. \n",
    "Curiosity piqued, she recalled an ancient wooden chest in her attic, untouched for decades. Climbing the creaky steps, she unlocked the chest \n",
    "to reveal a collection of letters penned by her grandmother. These letters unveiled stories of a hidden world filled with magical creatures and \n",
    "enchanted forests. As she read, the wind outside picked up, carrying whispers of the adventures her ancestors had once embarked upon. Willowbrook, \n",
    "it seemed, was not as ordinary as she had always believed.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"What did Ms. Agatha find in her attic?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33936-87d7-418a-be07-bbc59d311212",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    "---\n",
    "\n",
    "In the previous sections we saw a couple of things.\n",
    "\n",
    "- First, we did simple Q&A with the model\n",
    "- Second, we did some contextual QA with the model, where we gave it a piece of text (Document) and asked the model to answer questions from it.\n",
    "- Third, we went a bit further with the mechanism where we show some examples to the model as \"few shot\" and ask the question to the model.\n",
    "\n",
    "In the subsequent sections we will implement a RAG mechanism, step-by-step. RAG stands for \"Retriever-Augmented Generation\". It's a method in the domain of natural language processing (NLP) and information retrieval. RAG combines the powers of large pre-trained models like BERT (for information retrieval) and sequence-to-sequence models like BART or T5 (for generation) to produce answers to questions. Essentially, it retrieves relevant document passages from a corpus and then generates a response based on the information from those passages. To facilitate this, we will also take a look at vector databases, where we will store an entire document by first chunking it into smaller parts, and generating embeddings of those chunks, and finally loading them into the Vector DB. We will then see how we can do relevancy search on the Vecor DB to get text(s) relevant to our query, which will give us the basis of creating the context for the model. specifically, we will\n",
    "\n",
    "- Explore vector databases\n",
    "- Learn basics of QA exploring simple chains\n",
    "- Learn basics of chatbot\n",
    "- Build prompt templates for our chat bot\n",
    "- Explore various Chains useful for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff84015-923b-4e8f-b89a-4543f6755210",
   "metadata": {},
   "source": [
    "### Read the document\n",
    "\n",
    "Our final goal is to perform Q&A with the sample document `sagemaker-faqs.pdf`. First we need to read the text from the document for which we will use PyPDFLoader. We will then split this document into chunks, convert into embeddings and use with LangChain and SageMaker LLM for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524713c-5539-4e41-9f34-96cf4e78e993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"sagemaker-faqs.pdf\")\n",
    "documents_aws = loader.load() # -- gives 2 docs\n",
    "documents_split = loader.load_and_split() # - gives 22 docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb9c76-aeb4-4b31-80c0-803bfed0c417",
   "metadata": {},
   "source": [
    "We have split the document into smaller chunks. We will now perform a couple of things-\n",
    "\n",
    "- Generate embeddings of these chunks\n",
    "- Store these embeddings into a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8c530-e385-4129-8912-1d2677c1b52c",
   "metadata": {},
   "source": [
    "### Vector store indexer\n",
    "\n",
    "This is what stores and matches the embeddings. This notebook showcases FAISS and will be transient and in memory. FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions. The VectorStore APIs that use FAISS within LangChain are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html). You can read up about FAISS in memory vector store [here](https://arxiv.org/pdf/1702.08734.pdf).\n",
    "\n",
    "Some other notable Vector databases are\n",
    "\n",
    "- [Chroma](https://www.trychroma.com/) is a super simple vector search database. The core-API consists of just four functions, allowing users to build an in-memory document-vector store. By default Chroma uses the Hugging Face transformers library to vectorize documents.\n",
    "- [Weaviate](https://github.com/weaviate/weaviate) is a very posh looking tool - not only does Weaviate offer a GraphQL API with support for vector search. It also allows users to vectorize their content using Weaviate's inbuilt modules or custom modules.\n",
    "\n",
    "We will use `HuggingFaceEmbeddings` available via LangChain to generate embeddings of our text chunks that we generated in the previous step. This will be used by the FAISS (or Chroma) to store in memory and be used when ever the User runs a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad00af-6a7a-4857-aaa0-ace2a6a943d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_db = FAISS.from_documents(documents=documents_split, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65de05-7353-4ca8-adea-5ef68835e1db",
   "metadata": {},
   "source": [
    "We have loaded our vector db with the document, now let's run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f29a7-f04d-4840-9b3b-a2fb756d2bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How am I charged for sagemaker?\"\n",
    "docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5995cc8-ab61-499f-8ed4-357544051104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85230c-9805-4c0c-b7f2-f129626eece8",
   "metadata": {},
   "source": [
    "The query returns all the chunks from the document that is similar to the `query`, by default it returns the Top 3 similar chunks. Let's see how to return just Top 2 with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0c6d8-d3c0-4cfa-9a58-4b79b8c266cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vector_db.similarity_search_with_score(query, k = 2)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a53f54-eb46-41fe-a9e7-3696479f847d",
   "metadata": {},
   "source": [
    "### Vector store-backed retriever\n",
    "---\n",
    "\n",
    "According to LangChain documentation-\n",
    "\n",
    "> A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.\n",
    "\n",
    "Wrapping our vector db in a retriever wrapper is going to be useful when we use it in the Q&A chain for our chatbot in subsequent sections. But let's take a look how it works. The functionality is pretty similar to before (i.e. querying) with a slightly different interface.\n",
    "\n",
    "We first define a retriever with search type `mmr` (Max Marginal Relevance),  other option is `similarity`.  Note that the `search_type` depends on which vector DB you are using, some vector DBs may or may not support `mmr` etc. \n",
    "\n",
    "> MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document.\n",
    "\n",
    "We also define how many top results to return, in this case 2. Finally we query the retriever using `get_relevant_documents` by passing in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6144f13-3aab-4b90-9420-06778daaeea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How do I cost optimize sagemaker?\"\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 1})\n",
    "relevant_docs = retriever.get_relevant_documents(query)   \n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138c64b-c16b-47b7-9ab6-5def72fba116",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build context from retrieved documents\n",
    "---\n",
    "\n",
    "We now have the two relevant pieces of text that \"contain\" the anwer to our question, we are not quite there yet. So we will use a technique that we used earlier to build context and ask the quetion to the Llama-2 model. In this case, we will use the two text chunks we retrieved from the vector db to create the context by simply concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e71ca5-89a0-459b-ae2c-37c4c64e2eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_context = str()\n",
    "for doc in relevant_docs:\n",
    "    full_context += doc.page_content+\" \"\n",
    "    \n",
    "print(full_context.strip(\".\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e9560-7a2e-446a-abea-3c087a130d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": False,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e1248-279a-4190-a33d-8613ae3e04aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"Answer the question as truthfully as possible using the provided text. If the answer is not contained within the text below, say \"I don't know\", do not make up an answer.  \n",
    "# Text: {document}\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "template = \"\"\">>INTRODUCTION<<Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Make sure your answer is verbatim from the provided text. \n",
    ">>SUMMARY<<{document}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "query = \"How do I optimize sagemaker?\"\n",
    "\n",
    "# Run the chain with document as full_context and question as query we defined earlier\n",
    "output = llm_chain.run(document=full_context, question=query)\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49717aa-d417-4e1f-84f2-23e39fd2244c",
   "metadata": {
    "tags": []
   },
   "source": [
    "That's a much better and concise answer. Let's try another question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316c21d-c243-4fa9-bd16-1e9466c6003c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "query_1=\"How do I share models?\"\n",
    "output = llm_chain.run(document=full_context, question=query_1)\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885cf1cb-4f04-45da-a03e-4c8cce83c6fa",
   "metadata": {},
   "source": [
    "The model is unable to answer this specific question. That is because our `full_context` doesn't have any information related to the question. So we will have to again do a similarity search from the vector database to get the relevant chunks of text, then build the context with those chunks and then as the question to the LLM with that context. But that is a lot of repeated steps, and we can certainly write reusable functions to do it. However, there is a much easier way to achieve this using \"QA Chain\" available in LangChain, with just a few lines of code. So let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbcc7d-2109-474e-8f78-9bc09a967ee4",
   "metadata": {},
   "source": [
    "### Performing Q&A with RAG with `load_qa_chain`\n",
    "---\n",
    "\n",
    "For this purpose, we will first define a question, and then generate embeddings from it. Once we have that we can perform similarity search on the vector database to find relevant pieces of information from the document. These relevant pieces of information will then be passed on to the model so that it can answer the question. We will use LangChain's `load_qa_chain` to perform Q&A with the model. The load qa chain does the work with prompt creation and all the context generation with help from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c20c53-1902-4c03-9c39-16217af77f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 2})\n",
    "\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "template = \"\"\">>INTRODUCTION<<Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Make sure your answer is verbatim from the provided text. \n",
    ">>SUMMARY<<{context}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = { \"prompt\": qa_prompt }\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=sm_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "question=\"What are SageMaker Model cards?\"\n",
    "\n",
    "result = qa.run(question)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5cdc0",
   "metadata": {},
   "source": [
    "## Chatbot application\n",
    "\n",
    "#### For the chatbot we need `context management, history, vector stores, and many other things`. We will start by with a ConversationalRetrievalChain\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain which Allow for passing in chat history which can be used for follow up questions.Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "\n",
    "Set verbose to True to see all the what is going on behind the scenes\n",
    "\n",
    "**We use Custom Prompt template to fine tune the output responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42367663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=sm_llm, \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    retriever=retriever,\n",
    "    memory=memory_chain,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, # use the condense prompt template\n",
    "    #chain_type='map_reduce',\n",
    "    max_tokens_limit=100\n",
    "    #combine_docs_chain_kwargs=key_chain_args,\n",
    "\n",
    ")\n",
    "print(\"Starting chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdaaf4-17e6-45c8-acca-7c9905b8c556",
   "metadata": {},
   "source": [
    "#### Refine as Chain type with no similiarity searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cf2f5-345b-4473-af92-ea29de832e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \n",
    "\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=sm_llm, \n",
    "        retriever=vector_db.as_retriever(), \n",
    "        memory=memory_chain,\n",
    "        #verbose=True,\n",
    "        condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, create_prompt_template(), # use the condense prompt template\n",
    "        chain_type='refine', #'map_rerank', #'refine', # s(['stuff', 'map_reduce', 'refine', 'map_rerank'])\n",
    "        max_tokens_limit=100,\n",
    "        get_chat_history=lambda h : h,\n",
    ")  \n",
    "print(\"Starting Refine chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c1a75-4b49-45a2-8017-0487322ef547",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's delete the model and the endpoint to clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0f0d4-89a3-417b-9d4f-16646a4e4c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be1874-e6ab-4573-8ae8-da01c8e0fa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
